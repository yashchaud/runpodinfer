# vLLM Inference API Configuration
# Copy this file to .env and customize for your deployment

# ============================================
# ESSENTIAL CONFIGURATION
# ============================================

# Model to use - CHANGE THIS TO SWAP MODELS
MODEL_NAME=Qwen/Qwen2-VL-7B-Instruct

# HuggingFace token (required for private models, optional for public)
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=

# ============================================
# PERFORMANCE SETTINGS
# ============================================

# Maximum context length (reduce if Out of Memory)
# Qwen2-VL supports up to 128K, but 32K is recommended for 40GB GPU
MAX_MODEL_LEN=32768

# GPU memory utilization (0.5-0.99)
# Higher = more requests, but risk of OOM
GPU_MEMORY_UTIL=0.95

# Tensor parallelism (number of GPUs to split model across)
# Use for very large models (70B+)
TENSOR_PARALLEL_SIZE=1

# Data parallelism (number of independent model replicas)
# Use for scaling throughput with multiple GPUs
DATA_PARALLEL_SIZE=1

# Maximum concurrent sequences in a batch
# Higher = better throughput, more memory usage
MAX_NUM_SEQS=12

# Maximum tokens per batch
# Higher = better throughput, longer TTFT
# Lower = lower latency, lower throughput
MAX_BATCHED_TOKENS=10240

# ============================================
# MULTIMODAL LIMITS
# ============================================

# Maximum images per request
IMAGE_LIMIT=5

# Maximum videos per request
VIDEO_LIMIT=1

# Maximum audio files per request
AUDIO_LIMIT=0

# ============================================
# SERVER SETTINGS
# ============================================

# Server host (0.0.0.0 for external access)
HOST=0.0.0.0

# API port
PORT=8000

# vLLM internal port (should be different from PORT)
VLLM_PORT=8001

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Disable request logging for performance (true/false)
DISABLE_LOG_REQUESTS=false

# ============================================
# ADVANCED SETTINGS
# ============================================

# Enable chunked prefill for better TTFT (true/false)
ENABLE_CHUNKED_PREFILL=true

# Trust remote code (required for Qwen and some other models)
TRUST_REMOTE_CODE=true

# Attention backend: FLASH_ATTN, XFORMERS, TORCH_SDPA
VLLM_ATTENTION_BACKEND=FLASH_ATTN

# Image fetch timeout (seconds)
VLLM_IMAGE_FETCH_TIMEOUT=30

# Video fetch timeout (seconds)
VLLM_VIDEO_FETCH_TIMEOUT=60

# Engine iteration timeout (seconds)
VLLM_ENGINE_ITERATION_TIMEOUT_S=60

# Chat template file (optional, for custom models)
# CHAT_TEMPLATE=

# ============================================
# EXAMPLE CONFIGURATIONS
# ============================================

# High Throughput (sacrifice latency for throughput)
# MAX_BATCHED_TOKENS=20480
# MAX_NUM_SEQS=16

# Low Latency (sacrifice throughput for latency)
# MAX_BATCHED_TOKENS=2048
# MAX_NUM_SEQS=8

# Memory Constrained (reduce if OOM errors)
# MAX_MODEL_LEN=16384
# GPU_MEMORY_UTIL=0.85
# MAX_NUM_SEQS=6

# Multi-GPU Large Model (72B on 8x GPUs)
# MODEL_NAME=Qwen/Qwen2.5-VL-72B-Instruct
# TENSOR_PARALLEL_SIZE=8
# MAX_MODEL_LEN=65536
# MAX_NUM_SEQS=8
